{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61c2ccc8",
   "metadata": {},
   "source": [
    "(a) Sample 10 documents from TinyStories and OpenWebText. Using your previously-trained TinyS-\n",
    "tories and OpenWebText tokenizers (10K and 32K vocabulary size, respectively), encode these\n",
    "sampled documents into integer IDs. What is each tokenizer’s compression ratio (bytes/token)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a370d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer_class import BPE_Tokenizer\n",
    "import regex as re\n",
    "import random\n",
    "import numpy as np\n",
    "def sample_docs(dataset_path, sample_num, special_tokens):\n",
    "    special_tokens = sorted(special_tokens, key=lambda x: len(x), reverse=True)\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        text = f.read()\n",
    "    text_list = re.split(\"|\".join(special_tokens), text)\n",
    "    return random.sample(text_list, sample_num)\n",
    "\n",
    "def calculate_compression_ratio(text_list, tokenizer):\n",
    "    ratios = []\n",
    "    for text in text_list:\n",
    "        byte_count = len(text.encode('utf-8'))\n",
    "        encoded_count = len(tokenizer.encode(text))\n",
    "        if encoded_count > 0:\n",
    "            ratios.append(byte_count / encoded_count)\n",
    "    return np.mean(ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bec9e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dataset_path = \"/data/xqjiao/cs336/assignments/assignment1-basics/data/TinyStoriesV2-GPT4-valid.txt\"\n",
    "vocab_path = \"/data/xqjiao/cs336/assignments/assignment1-basics/output/bpe_tokenizers/TinyStoriesV2-GPT4-train/vocab.pkl\"\n",
    "merges_path = \"/data/xqjiao/cs336/assignments/assignment1-basics/output/bpe_tokenizers/TinyStoriesV2-GPT4-train/merges.pkl\"\n",
    "special_tokens = ['<|endoftext|>']\n",
    "ts_tokenizer = BPE_Tokenizer.from_files(vocab_filepath=vocab_path, merges_filepath=merges_path, special_tokens=special_tokens)\n",
    "\n",
    "owt_dataset_path = \"/data/xqjiao/cs336/assignments/assignment1-basics/data/owt_valid.txt\"\n",
    "vocab_path = \"/data/xqjiao/cs336/assignments/assignment1-basics/output/bpe_tokenizers/owt_train/vocab.pkl\"\n",
    "merges_path = \"/data/xqjiao/cs336/assignments/assignment1-basics/output/bpe_tokenizers/owt_train/merges.pkl\"\n",
    "special_tokens = ['<|endoftext|>']\n",
    "owt_tokenizer = BPE_Tokenizer.from_files(vocab_filepath=vocab_path, merges_filepath=merges_path, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd48327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tinystories\n",
    "sample_num = 10\n",
    "sampled_texts_ts = sample_docs(ts_dataset_path, sample_num, special_tokens)\n",
    "compression_ratio = calculate_compression_ratio(sampled_texts_ts, ts_tokenizer)\n",
    "print(compression_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a5c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# owt\n",
    "sample_num = 10\n",
    "sampled_texts_owt = sample_docs(owt_dataset_path, sample_num, special_tokens)\n",
    "compression_ratio = calculate_compression_ratio(sampled_texts_owt, owt_tokenizer)\n",
    "print(compression_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b789c26",
   "metadata": {},
   "source": [
    "What happens if you tokenize your OpenWebText sample with the TinyStories tokenizer? Com-\n",
    "pare the compression ratio and/or qualitatively describe what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75569d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_ratio = calculate_compression_ratio(sampled_texts_owt, ts_tokenizer)\n",
    "print(compression_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55fd06e",
   "metadata": {},
   "source": [
    "Using your TinyStories and OpenWebText tokenizers, encode the respective training and devel-\n",
    "opment datasets into a sequence of integer token IDs. We’ll use this later to train our language\n",
    "model. We recommend **serializing the token IDs as a NumPy array of datatype uint16**. Why is\n",
    "uint16 an appropriate choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872718eb",
   "metadata": {},
   "source": [
    "arr2 = np.array([10, 20, 30])\n",
    "arr2_uint16 = arr2.astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2759a6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import BinaryIO, Iterable, List, Tuple\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "def find_chunk_boundaries(\n",
    "    file: BinaryIO,\n",
    "    desired_num_chunks: int,\n",
    "    split_special_token: bytes,\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Chunk the file into parts that can be counted independently.\n",
    "    May return fewer chunks if the boundaries end up overlapping.\n",
    "    \"\"\"\n",
    "    assert isinstance(split_special_token, bytes), \"Must represent special token as a bytestring\"\n",
    "\n",
    "    # Get total file size in bytes\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()\n",
    "    file.seek(0)\n",
    "\n",
    "    chunk_size = file_size // desired_num_chunks\n",
    "\n",
    "    # Initial guesses for chunk boundary locations, uniformly spaced\n",
    "    # Chunks start on previous index, don't include last index\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "\n",
    "    # chunk_count = len(chunk_boundaries) - 1\n",
    "    chunk_boundaries[-1] = file_size\n",
    "\n",
    "    mini_chunk_size = 4096*2  # Read ahead by 4k*2 bytes at a time\n",
    "\n",
    "    for bi in range(1, len(chunk_boundaries) - 1):\n",
    "        initial_position = chunk_boundaries[bi]\n",
    "        file.seek(initial_position)  # Start at boundary guess\n",
    "        while True:\n",
    "            mini_chunk = file.read(mini_chunk_size)  # Read a mini chunk\n",
    "\n",
    "            # If EOF, this boundary should be at the end of the file\n",
    "            if mini_chunk == b\"\":\n",
    "                chunk_boundaries[bi] = file_size\n",
    "                break\n",
    "\n",
    "            # Find the special token in the mini chunk\n",
    "            found_at = mini_chunk.find(split_special_token)\n",
    "            if found_at != -1:\n",
    "                chunk_boundaries[bi] = initial_position + found_at\n",
    "                break\n",
    "            initial_position += mini_chunk_size\n",
    "\n",
    "    # Make sure all boundaries are unique, but might be fewer than desired_num_chunks\n",
    "\n",
    "    return sorted(set(chunk_boundaries))\n",
    "\n",
    "def encode_dataset(dataset_path, tokenizer, output_path, special_tokens):\n",
    "    output = []\n",
    "    with open(dataset_path, 'rb') as f:\n",
    "        boundaries = find_chunk_boundaries(f, desired_num_chunks=8, split_special_token=b\"<|endoftext|>\")\n",
    "        start_end_pairs = [(boundaries[i], boundaries[i+1]) for i in range(len(boundaries)-1)]\n",
    "        chunk_list = []\n",
    "        for start, end in start_end_pairs:\n",
    "            f.seek(start)\n",
    "            chunk = f.read(end - start).decode('utf-8', errors='ignore')\n",
    "            chunk_list.append(chunk)\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        results = pool.starmap(tokenizer.encode, [(chunk,) for chunk in chunk_list])\n",
    "    for res in results:\n",
    "        output.extend(res)\n",
    "    \n",
    "    dataset_name = dataset_path.split('/')[-1].split('.')[0]\n",
    "    print(f\"{dataset_name} dataset encoded, total length: {len(output)}\")\n",
    "    print(f\"saved to {output_path}\")\n",
    "    np.save(output_path, np.array(output, dtype=np.uint16))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2dbf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_train_dataset_path = \"/data/xqjiao/cs336/assignments/assignment1-basics/data/TinyStoriesV2-GPT4-train.txt\"\n",
    "ts_valid_dataset_path = \"/data/xqjiao/cs336/assignments/assignment1-basics/data/TinyStoriesV2-GPT4-valid.txt\"\n",
    "owt_train_dataset_path = \"/data/xqjiao/cs336/assignments/assignment1-basics/data/owt_train.txt\"\n",
    "owt_valid_dataset_path = \"/data/xqjiao/cs336/assignments/assignment1-basics/data/owt_valid.txt\"\n",
    "\n",
    "special_tokens = ['<|endoftext|>']\n",
    "\n",
    "output_dir = \"/data/xqjiao/cs336/assignments/assignment1-basics/output/encoded_datasets\"\n",
    "\n",
    "\n",
    "encode_dataset(ts_valid_dataset_path, ts_tokenizer, os.path.join(output_dir, \"ts_valid.npy\"), special_tokens)\n",
    "encode_dataset(ts_train_dataset_path, ts_tokenizer, os.path.join(output_dir, \"ts_train.npy\"), special_tokens)\n",
    "# encode_dataset(owt_train_dataset_path, owt_tokenizer, os.path.join(output_dir, \"owt_train.npy\"), special_tokens)\n",
    "# encode_dataset(owt_valid_dataset_path, owt_tokenizer, os.path.join(output_dir, \"owt_valid.npy\"), special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d44b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ts_tokenizer.encode(\"Hello, world! This is a test. <|endoftext|> Another sentence.\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06172521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cefce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7717a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2b7a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c53038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d025bbdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcd4934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
