{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69919716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n",
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "chr(0)\n",
    "print(chr(0))\n",
    "print(\"this is a test\" + chr(0) + \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6a5c51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'dog.   ??q()$%$$\\xc2\\xa5'\n"
     ]
    }
   ],
   "source": [
    "string = \"dog.   ??q()$%$$¥\"\n",
    "encode_8 = string.encode('utf-8')\n",
    "encode_16 = string.encode('utf-16')\n",
    "print(encode_8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75569d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平方结果: [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n",
      "5的平方: 25\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "def square(x):\n",
    "    \"\"\"计算平方\"\"\"\n",
    "    return x * x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 创建进程池，使用所有可用的CPU核心\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        # map方法：将函数应用到序列的每个元素\n",
    "        results = pool.map(square, range(10))\n",
    "        print(f\"平方结果: {results}\")\n",
    "        \n",
    "        # apply_async方法：异步执行\n",
    "        result = pool.apply_async(square, (5,))\n",
    "        print(f\"5的平方: {result.get()}\")  # 使用get()获取结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d2dbf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe'}\n"
     ]
    }
   ],
   "source": [
    "current_vocab_size = 0\n",
    "vocab = {}\n",
    "for i in range(0, 255):\n",
    "    vocab[current_vocab_size] = bytes([i])\n",
    "    current_vocab_size += 1\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b4d44b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token = \"<sqws你>\"\n",
    "special_token = special_token.encode('utf-8')\n",
    "tokens = [bytes([b]) for b in special_token]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06172521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, [2]]\n"
     ]
    }
   ],
   "source": [
    "a = [1]\n",
    "b = [2]\n",
    "c= a + [b]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33cefce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "a = (b' t', b'h')\n",
    "b = (b'i', b'n')\n",
    "\n",
    "print(a>b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c7717a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((b'i', b'n'), 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dict_ = {(b' t', b'h'): 2, (b'i', b'n'): 2}\n",
    "\n",
    "print(max(dict_.items(), key = lambda item:(item[1], item[0]))  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc2b7a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "a = b'abc'\n",
    "b = b'ssabcabcdabc'\n",
    "print(b.find(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1c53038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "def find_all_occurrences(main_str, sub_str):\n",
    "    indexes = []\n",
    "    start = 0 \n",
    "    sub_len = len(sub_str)\n",
    "    main_len = len(main_str)\n",
    "    \n",
    "    if sub_len == 0 or sub_len > main_len:\n",
    "        return indexes\n",
    "    \n",
    "    while True:\n",
    "        pos = main_str.find(sub_str, start)\n",
    "        if pos == -1:  \n",
    "            break\n",
    "        indexes.append(pos)\n",
    "        start = pos + 1\n",
    "    return indexes  \n",
    "\n",
    "\n",
    "main_str = b' here'\n",
    "sub_str = b're'\n",
    "print(find_all_occurrences(main_str, sub_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d025bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import BinaryIO, Iterable, List, Tuple\n",
    "import regex as re\n",
    "import multiprocessing\n",
    "\n",
    "def find_chunk_boundaries(\n",
    "    file: BinaryIO,\n",
    "    desired_num_chunks: int,\n",
    "    split_special_token: bytes,\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Chunk the file into parts that can be counted independently.\n",
    "    May return fewer chunks if the boundaries end up overlapping.\n",
    "    \"\"\"\n",
    "    assert isinstance(split_special_token, bytes), \"Must represent special token as a bytestring\"\n",
    "\n",
    "    # Get total file size in bytes\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()\n",
    "    file.seek(0)\n",
    "\n",
    "    chunk_size = file_size // desired_num_chunks\n",
    "\n",
    "    # Initial guesses for chunk boundary locations, uniformly spaced\n",
    "    # Chunks start on previous index, don't include last index\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "\n",
    "    # chunk_count = len(chunk_boundaries) - 1\n",
    "    chunk_boundaries[-1] = file_size\n",
    "\n",
    "    mini_chunk_size = 4096  # Read ahead by 4k bytes at a time\n",
    "\n",
    "    for bi in range(1, len(chunk_boundaries) - 1):\n",
    "        initial_position = chunk_boundaries[bi]\n",
    "        file.seek(initial_position)  # Start at boundary guess\n",
    "        while True:\n",
    "            mini_chunk = file.read(mini_chunk_size)  # Read a mini chunk\n",
    "\n",
    "            # If EOF, this boundary should be at the end of the file\n",
    "            if mini_chunk == b\"\":\n",
    "                chunk_boundaries[bi] = file_size\n",
    "                break\n",
    "\n",
    "            # Find the special token in the mini chunk\n",
    "            found_at = mini_chunk.find(split_special_token)\n",
    "            if found_at != -1:\n",
    "                chunk_boundaries[bi] = initial_position + found_at\n",
    "                break\n",
    "            initial_position += mini_chunk_size\n",
    "\n",
    "    # Make sure all boundaries are unique, but might be fewer than desired_num_chunks\n",
    "\n",
    "    return sorted(set(chunk_boundaries))\n",
    "\n",
    "\n",
    "def pretokenize(chunk: str, special_tokens: list[str]) -> dict[str, int]:\n",
    "    # Removing special tokens before pre-tokenization\n",
    "    # This can be done using re.split with \"|\".join(special_tokens) as the delimiter \n",
    "    text_list = re.split(\"|\".join(special_tokens), chunk)\n",
    "    pattern = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "    pre_tokens = {}\n",
    "    for chunk in text_list:\n",
    "        find_words = re.findall(pattern, chunk)\n",
    "        for word in find_words:\n",
    "            # word = word.strip()\n",
    "            pre_tokens[word] = 1 + pre_tokens.get(word, 0)\n",
    "    return pre_tokens\n",
    "\n",
    "\n",
    "def train_bpe_tokenizer(\n",
    "    input_path: str,\n",
    "    vocab_size: int,\n",
    "    special_tokens: list[str]\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    vocab: dict[int, bytes] The tokenizer vocabulary, a mapping from int (token ID in the vocabulary) to bytes (token bytes).\n",
    "    \n",
    "    merges: list[tuple[bytes, bytes]] A list of BPE merges produced from training. Each list item\n",
    "    is a tuple of bytes (<token1>, <token2>), representing that <token1> was merged with\n",
    "    <token2>. The merges should be ordered by order of creation.\n",
    "    \"\"\"\n",
    "    vocab: dict[int, bytes] = {} # dict[int, bytes]\n",
    "    merges: list[tuple[bytes, bytes]] = [] \n",
    "\n",
    "    token_id = 0\n",
    "    for i in range(256):\n",
    "        vocab[token_id] = bytes([i])\n",
    "        token_id += 1\n",
    "\n",
    "    for token in special_tokens:\n",
    "        vocab[token_id] = token.encode(\"utf-8\")\n",
    "        token_id += 1\n",
    "    \n",
    "    num_processes = os.cpu_count()\n",
    "\n",
    "    pre_token_count: dict[str, int] = {}\n",
    "    pre_token_list: list[str] = []\n",
    "\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        boundaries = find_chunk_boundaries(f, num_processes, b\"<|endoftext|>\")\n",
    "        start_end_pairs = list(zip(boundaries[:-1], boundaries[1:]))\n",
    "        chunk_list = []\n",
    "        for start, end in start_end_pairs:\n",
    "            f.seek(start)\n",
    "            chunk = f.read(end - start).decode('utf-8', errors='ignore')\n",
    "            chunk_list.append(chunk)\n",
    "        num_processes = min(num_processes, len(chunk_list))\n",
    "        with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "            results = pool.starmap(pretokenize, [(chunk, special_tokens) for chunk in chunk_list])\n",
    "            for res in results:\n",
    "                for token, count in res.items():\n",
    "                    if token not in pre_token_count:\n",
    "                        pre_token_list.append(token)\n",
    "                    pre_token_count[token] = pre_token_count.get(token, 0) + count\n",
    "\n",
    "    # Initialize token_sequences for each pre-token string\n",
    "    token_sequences = []\n",
    "    for pre_token in pre_token_list:\n",
    "        bytes_data = pre_token.encode('utf-8')\n",
    "        tokens = [bytes([b]) for b in bytes_data]\n",
    "        token_sequences.append(tokens)\n",
    "\n",
    "    # Initialize token_pair_count and token_pair_to_indices\n",
    "    token_pair_count = {}\n",
    "    token_pair_to_indices = {}\n",
    "    for i, tokens in enumerate(token_sequences):\n",
    "        count_i = pre_token_count[pre_token_list[i]]\n",
    "        for j in range(len(tokens)-1):\n",
    "            pair = (tokens[j], tokens[j+1])\n",
    "            token_pair_count[pair] = token_pair_count.get(pair, 0) + count_i\n",
    "            if pair not in token_pair_to_indices:\n",
    "                token_pair_to_indices[pair] = set()\n",
    "            token_pair_to_indices[pair].add(i)\n",
    "\n",
    "    # Train BPE tokenizer\n",
    "    while len(vocab) < vocab_size:\n",
    "        if not token_pair_count:\n",
    "            break\n",
    "        # Find the most frequent token pair\n",
    "        pair = max(token_pair_count, key=token_pair_count.get)\n",
    "        count = token_pair_count[pair]\n",
    "        if count < 2:\n",
    "            break\n",
    "\n",
    "        merges.append(pair)\n",
    "        new_token = pair[0] + pair[1]\n",
    "        current_id = len(vocab)\n",
    "        vocab[current_id] = new_token\n",
    "\n",
    "        # Get all indices that contain this pair\n",
    "        indices = token_pair_to_indices.get(pair, set()).copy()\n",
    "        for i in indices:\n",
    "            tokens = token_sequences[i]\n",
    "            new_tokens = []\n",
    "            j = 0\n",
    "            while j < len(tokens):\n",
    "                if j < len(tokens)-1 and tokens[j] == pair[0] and tokens[j+1] == pair[1]:\n",
    "                    new_tokens.append(new_token)\n",
    "                    j += 2\n",
    "                else:\n",
    "                    new_tokens.append(tokens[j])\n",
    "                    j += 1\n",
    "\n",
    "            # Update token pairs for this pre-token string\n",
    "            count_i = pre_token_count[pre_token_list[i]]\n",
    "\n",
    "            # Remove all old pairs from the original token sequence\n",
    "            for k in range(len(tokens)-1):\n",
    "                old_pair = (tokens[k], tokens[k+1])\n",
    "                if old_pair in token_pair_count:\n",
    "                    token_pair_count[old_pair] -= count_i\n",
    "                    if token_pair_count[old_pair] <= 0:\n",
    "                        del token_pair_count[old_pair]\n",
    "                if old_pair in token_pair_to_indices:\n",
    "                    token_pair_to_indices[old_pair].discard(i)\n",
    "                    if not token_pair_to_indices[old_pair]:\n",
    "                        del token_pair_to_indices[old_pair]\n",
    "\n",
    "            # Add all new pairs from the new token sequence\n",
    "            for k in range(len(new_tokens)-1):\n",
    "                new_pair = (new_tokens[k], new_tokens[k+1])\n",
    "                token_pair_count[new_pair] = token_pair_count.get(new_pair, 0) + count_i\n",
    "                if new_pair not in token_pair_to_indices:\n",
    "                    token_pair_to_indices[new_pair] = set()\n",
    "                token_pair_to_indices[new_pair].add(i)\n",
    "\n",
    "            # Update the token sequence for this index\n",
    "            token_sequences[i] = new_tokens\n",
    "\n",
    "        # Remove the merged pair from token_pair_count and token_pair_to_indices\n",
    "        if pair in token_pair_count:\n",
    "            del token_pair_count[pair]\n",
    "        if pair in token_pair_to_indices:\n",
    "            del token_pair_to_indices[pair]\n",
    "\n",
    "    return vocab, merges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcd4934",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # del token_pair_count[merge_pair]\n",
    "        # del token_pair_positions[merge_pair]\n",
    "        # # position is the index of pre_token_list, counts the occurences of merge_pair in the pre_token\n",
    "        # for pos, _ in merge_positions.items():\n",
    "        #     pre_token = pre_token_list[pos]\n",
    "        #     bytes_data = pre_token.encode('utf-8')\n",
    "        #     count = pre_token_count[pre_token]\n",
    "        #      # get the occur positions of merge_pair in the pre_token\n",
    "        #     tokens = [bytes([b]) for b in bytes_data]\n",
    "        #     occur_pos = find_all_occurrences(bytes_data, new_token)\n",
    "        #     # del left pair and right pair, add new left pair and new right pair\n",
    "        #     for start in occur_pos:\n",
    "        #         if start > 0:\n",
    "        #             left_token = tokens[start-1]\n",
    "        #             i = start - 2\n",
    "        #             while i > 0:\n",
    "        #                 possible_left_token = tokens[i] + left_token\n",
    "        #                 if possible_left_token in vocab.values():\n",
    "        #                     left_token = possible_left_token\n",
    "        #                     i -= 1\n",
    "        #                 else:\n",
    "        #                     break\n",
    "        #             left_pair = (left_token, merge_pair[0])\n",
    "        #             if left_pair in token_pair_count:\n",
    "        #                 token_pair_count[left_pair] -= count\n",
    "        #                 if token_pair_count[left_pair] <= 0:\n",
    "        #                     del token_pair_count[left_pair]\n",
    "        #                     del token_pair_positions[left_pair]\n",
    "        #                 elif pos in token_pair_positions[left_pair]:\n",
    "        #                     token_pair_positions[left_pair][pos] -= 1\n",
    "                            \n",
    "        #             new_pair = (left_token, new_token)\n",
    "        #             token_pair_count[new_pair] = token_pair_count.get(new_pair, 0) + count\n",
    "        #             if new_pair not in token_pair_positions:\n",
    "        #                 token_pair_positions[new_pair] = {}\n",
    "        #             token_pair_positions[new_pair][pos] = token_pair_positions[new_pair].get(pos, 0) + 1\n",
    "\n",
    "        #             # if pre_token == \" here\" and new_token == b're':\n",
    "        #             #     print(\"here........\")\n",
    "        #             #     print('new_pair:', new_pair)\n",
    "        #             #     print(token_pair_positions[new_pair])\n",
    "\n",
    "        #         if start + len(merge_pair[0]) + len(merge_pair[1]) < len(tokens):\n",
    "        #             # right_pair = (merge_pair[1], tokens[start + len(merge_pair[0])+ len(merge_pair[1])])\n",
    "        #             right_token = tokens[start + len(merge_pair[0]) + len(merge_pair[1])]\n",
    "        #             i = start + len(merge_pair[0]) + len(merge_pair[1]) + 1\n",
    "        #             while i < len(tokens):\n",
    "        #                 possible_right_token = right_token + tokens[i]\n",
    "        #                 if possible_right_token in vocab.values() and possible_right_token != new_token:\n",
    "        #                     right_token = possible_right_token\n",
    "        #                     i += 1\n",
    "        #                 else:\n",
    "        #                     break\n",
    "        #             right_pair = (merge_pair[1], right_token)\n",
    "        #             if right_pair in token_pair_count:\n",
    "        #                 token_pair_count[right_pair] -= count\n",
    "        #                 if token_pair_count[right_pair] <= 0:\n",
    "        #                     del token_pair_count[right_pair]\n",
    "        #                     del token_pair_positions[right_pair]\n",
    "        #                 elif pos in token_pair_positions[right_pair]:\n",
    "        #                     token_pair_positions[right_pair][pos] -= 1\n",
    "        #             new_pair = (new_token, right_token)\n",
    "        #             token_pair_count[new_pair] = token_pair_count.get(new_pair, 0) + count\n",
    "        #             if new_pair not in token_pair_positions:\n",
    "        #                 token_pair_positions[new_pair] = {}\n",
    "        #             token_pair_positions[new_pair][pos] = token_pair_positions[new_pair].get(pos, 0) + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
